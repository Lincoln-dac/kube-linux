1. 确保K8S版本升级到1.18及以上(依赖特性：TopologyManager)
https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/

2. 增加kubelet启动参数：
--cpu-manager-policy=static 
--topology-manager-policy=single-numa-node

1）修改配置文件 /etc/kubernetes/kubelet


2）修改配置文件 /usr/lib/systemd/system/kubelet.service


3）重启kubelet:  systemctl daemon-reload && systemctl restart kubelet


4）检查，确保启动参数生效


3. 验证numa亲和性
1）确认节点CPU 分布情况：
NUMA node0 CPU(s):     0-23,48-71
NUMA node1 CPU(s):     24-47,72-95


2）先后创建三个static类型（request和limit严格一致）的Pod：
debug1： CPU request==limit==40C
debug2： CPU request==limit==40C
debug3： CPU request==limit==10C

实验预期：
➢ debug1与debug2分布在不同的numa上，各自占用40C CPU资源，numa1与numa2各自剩余8C。
➢ debug3预期需要10C并且都在一个numa上，在debug1和debug2各自占用40C的情况下，总共剩余16C CPU，但每个numa剩余8C<10C，debug3必定调度失败。

3）验证
debug1上创建40个100%使用CPU的进程，查看进程分布情况：debug1全部分布在numa0上





同样，debug2全部分布在numa1上。

debug3由于没有numa满足>=10C，调度失败。



4. 确保Pod内的进程在本numa分配内存
本质上是通过系统调用(set_mempolicy)设置进程属性,在内核给进程分配内存时，内核只在进程所属numa分配内存。如果业务进程本身没有明显调用set_mempolicy设置内存分配策略，可以通过numactl --localalloc cmd 启动的进程，内核分配内存时会严格保证内存分布在本numa 

总结：
1）numa亲和性调度需要K8S 1.18及以上版本，在对应节点增加kubelet启动参数：
--cpu-manager-policy=static 
--topology-manager-policy=single-numa-node
开启粒度为节点级别。
2）可以配合K8S节点标签，给对应节点开启numa亲和性调度，把业务Pod调度到numa亲和性	节点，kubelet会严格把对应业务pod做numa亲和性绑定，确保分配的CPU都在一个numa上
3）Pod 对CPU request 和 limit配置一致 

参考文档：
https://cloud.tencent.com/developer/article/1402119 
https://zhuanlan.zhihu.com/p/33621500
