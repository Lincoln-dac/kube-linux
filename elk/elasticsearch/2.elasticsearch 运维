GET  /_cat
_cat系列提供了一系列查询elasticsearch集群状态的接口。你可以通过执行
curl -XGET localhost:9200/_cat
获取所有_cat系列的操作
/_cat/allocation
/_cat/shards 查看所有分片状态
/_cat/shards/{index}
/_cat/master 查看主节点
/_cat/nodes 查看节点
/_cat/indices 查看索引
/_cat/indices/{index}
/_cat/segments
/_cat/segments/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
/_cat/plugins
/_cat/fielddata
/_cat/fielddata/{fields}
你也可以后面加一个v，让输出内容表格显示表头，
\?pretty 人性化显示

 curl -XGET http://10.204.51.64:9200/_cluster/health\?pretty


查看集群状态
   
显示集群状态
curl -XGET http://10.204.243.82:9200/_cluster/health
curl -XGET http://10.204.243.82:9200/_cluster/state
curl -XGET http://10.204.243.82:9200/_cat/nodes
cluster_name：表示集群名称
status：用来标识集群健康状况，green-健康，yellow-亚健康，red-病态，具体含义见后面
number_of_nodes：节点数量，包括master、data、client节点
number_of_data_nodes：data节点数量
active_primary_shards：活跃的主分片数目
active_shards：活跃的分片数，包括主、从索引的分片
--------------
green：所有的主分片和副本分片都已分配。你的集群是 100% 可用的。
yellow：所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。
red：至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。

显示集群系统信息，包括CPU JVM等等
curl -XGET http://10.204.243.82:9200/_cluster/stats?pretty=true
显示集群的详细信息。包括节点、分片等。
curl -XGET 'http://10.204.243.82:9200/_cluster/state?pretty'
获取集群堆积的任务
curl -XGET 'http://10.204.243.82:9200/_cluster/pending_tasks?pretty=true'
修改集群配置
举例：
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}'
transient 表示临时的，persistent表示永久的


关闭节点

1.关闭指定192.168.1.1节点
curl -XPOST ‘http://192.168.1.1:9200/_cluster/nodes/_local/_shutdown’
curl -XPOST ‘http://localhost:9200/_cluster/nodes/192.168.1.1/_shutdown’
2.关闭主节点
curl -XPOST ‘http://localhost:9200/_cluster/nodes/_master/_shutdown’
3.关闭整个集群
curl -XPOST ‘http://localhost:9200/_shutdown?delay=10s’
curl -XPOST ‘http://localhost:9200/_cluster/nodes/_shutdown’
curl -XPOST ‘http://localhost:9200/_cluster/nodes/_all/_shutdown’


nodes系列
1、查询节点的状态
curl -XGET ‘http://localhost:9200/_nodes/stats?pretty=true’
curl -XGET ‘http://localhost:9200/_nodes/192.168.1.2/stats?pretty=true’
curl -XGET ‘http://localhost:9200/_nodes/process’
curl -XGET ‘http://localhost:9200/_nodes/_all/process’
curl -XGET ‘http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/jvm,process’
curl -XGET ‘http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/info/jvm,process’
curl -XGET ‘http://localhost:9200/_nodes/192.168.1.2,192.168.1.3/_all
curl -XGET ‘http://localhost:9200/_nodes/hot_threads

/_nodes/process	我主要看file descriptor 这个信息
/_nodes/process/stats	统计信息（内存、CPU能）
/_nodes/jvm	获得各节点的虚拟机统计和配置信息
/_nodes/jvm/stats	更详细的虚拟机信息
/_nodes/http	获得各个节点的http信息（如ip地址）
/_nodes/http/stats	获得各个节点处理http请求的统计情况
/_nodes/thread_pool	获得各种类型的线程池（elasticsearch分别对不同的操作提供不同的线程池）的配置信息
/_nodes/thread_pool/stats	获得各种类型的线程池的统计信息

2.节点下线
curl -XPUT 127.0.0.1:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
   }
}'

索引操作
1、获取索引
curl -XGET ‘http://localhost:9200/{index}/{type}/{id}’
2、索引数据
curl -XPOST ‘http://localhost:9200/{index}/{type}/{id}’ -d'{“a”:”avalue”,”b”:”bvalue”}’
3、删除索引
curl -XDELETE ‘http://localhost:9200/{index}/{type}/{id}’
4、创建索引
curl -XPUT 'http://localhost:9200/vp
5、获取所有索引
curl -XGET ‘http://localhost:9200/_cat/indices?v'
health：表示健康状况
status：索引的状态，open-索引已开启，close-索引已关闭。关闭索引表示锁定了读写操作
pri：索引的主分片的数目
rep：索引的副本数
docs.count：索引包含的文档数目
docs.deleted：索引中已删除的文档数
store.size：索引占用的存储空间，包括主分片及其副本
pri.store.size：索引主分片占用的存储空间
6、关闭索引
curl -XPUT 'http:///localhost:9200/:9200/fvp/_close'
7、开启索引
curl -XPUT 'http:///localhost:9200/:9200/fvp/_open'

/index/_search	不解释
/_aliases	获取或操作索引的别名
/index/	 
/index/type/	创建或操作类型
/index/_mapping	创建或操作mapping
/index/_settings	创建或操作设置(number_of_shards是不可更改的)
/index/_open	打开被关闭的索引
/index/_close	关闭索引
/index/_refresh	刷新索引（使新加内容对搜索可见）
/index/_flush	刷新索引 将变动提交到lucene索引文件中 并清空elasticsearch的transaction log，与refresh的区别需要继续研究
/index/_optimize	优化segement，个人认为主要是对segement进行合并
/index/_status	获得索引的状态信息
/index/_segments	获得索引的segments的状态信息
/index/_explain	不执行实际搜索，而返回解释信息
/index/_analyze	不执行实际搜索，根据输入的参数进行文本分析
/index/type/id	操作指定文档，不解释
/index/type/id/_create	创建一个文档，如果该文件已经存在，则返回失败
/index/type/id/_update	更新一个文件，如果改文件不存在，则返回失败




集群自动发现
fd 是 fault detection
考虑到节点有时候因为高负载，慢 GC [垃圾回收] 等原因会偶尔没及时响应 ping ,一般建议稍加大 Fault Detection 的超时时间。
discovery.zen.ping_timeout 仅在加入或者选举 master 主节点的时候才起作用；
discovery.zen.fd.ping_timeout 在稳定运行的集群中，master检测所有节点，以及节点检测 master是否畅通时长期有用
discovery.zen.ping.unicast.hosts: ["es0","es1", "es2","es3","es4"]    # 集群自动发现
discovery.zen.fd.ping_timeout: 120s                # 超时时间(根据实际情况调整)
discovery.zen.fd.ping_retries: 6                   # 重试次数，防止GC[Garbage collection]节点不响应被剔除
discovery.zen.fd.ping_interval: 30s                # 运行间隔

索引分片： 从策略层面，控制分片分配的选择
磁盘限额 为了保护节点数据安全，ES 会定时(cluster.info.update.interval，默认 30 秒)检查一下各节点的数据目录磁盘使用情况。在达到 cluster.routing.allocation.disk.watermark.low (默认 85%)的时候，新索引分片就不会再分配到这个节点上了。在达到 cluster.routing.allocation.disk.watermark.high (默认 90%)的时候，就会触发该节点现存分片的数据均衡，把数据挪到其他节点上去。这两个值不但可以写百分比，还可以写具体的字节数。有些公司可能出于成本考虑，对磁盘使用率有一定的要求，需要适当抬高这个配置：
# curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.watermark.low" : "85%",
        "cluster.routing.allocation.disk.watermark.high" : "10gb",
        "cluster.info.update.interval" : "1m"
    }
}'

热索引分片不均 默认情况下，ES 集群的数据均衡策略是以各节点的分片总数(indices_all_active)作为基准的。这对于搜索服务来说无疑是均衡搜索压力提高性能的好办法。但是对于 Elastic Stack 场景，一般压力集中在新索引的数据写入方面。正常运行的时候，也没有问题。但是当集群扩容时，新加入集群的节点，分片总数远远低于其他节点。这时候如果有新索引创建，ES 的默认策略会导致新索引的所有主分片几乎全分配在这台新节点上。整个集群的写入压力，压在一个节点上，结果很可能是这个节点直接被压死，集群出现异常。 所以，对于 Elastic Stack 场景，强烈建议大家预先计算好索引的分片数后，配置好单节点分片的限额。比如，一个 5 节点的集群，索引主分片 10 个，副本 1 份。则平均下来每个节点应该有 4 个分片，那么就配置：
# curl -s -XPUT http://127.0.0.1:9200/logstash-2015.05.08/_settings -d '{
    "index": { "routing.allocation.total_shards_per_node" : "5" }
}'

注意，这里配置的是 5 而不是 4。因为我们需要预防有机器故障，分片发生迁移的情况。如果写的是 4，那么分片迁移会失败。 
此外，另一种方式则更加玄妙，Elasticsearch 中有一系列参数，相互影响，最终联合决定分片分配： 
cluster.routing.allocation.balance.shard 节点上分配分片的权重，默认为 0.45。数值越大越倾向于在节点层面均衡分片。 
cluster.routing.allocation.balance.index 每个索引往单个节点上分配分片的权重，默认为 0.55。数值越大越倾向于在索引层面均衡分片。 
cluster.routing.allocation.balance.threshold 大于阈值则触发均衡操作。默认为1。

GC(垃圾回收)
对不了解 JVM 的 GC 的读者，这里先介绍一下 GC(垃圾收集)以及 GC 对 Elasticsearch 的影响。 
Java is a garbage-collected language, which means that the programmer does not manually manage memory allocation and deallocation. The programmer simply writes code, and the Java Virtual Machine (JVM) manages the process of allocating memory as needed, and then later cleaning up that memory when no longer needed. Java 是一个自动垃圾收集的编程语言，启动 JVM 虚拟机的时候，会分配到固定大小的内存块，这个块叫做 heap(堆)。JVM 会把 heap 分成两个组： 
Young 新实例化的对象所分配的空间。这个空间一般来说只有 100MB 到 500MB 大小。Young 空间又分为两个 survivor(幸存)空间。当 Young 空间满，就会发生一次 young gc，还存活的对象，就被移入幸存空间里，已失效的对象则被移除。 
Old 老对象存储的空间。这些对象应该是长期存活而且在较长一段时间内不会变化的内容。这个空间会大很多，在 ES 来说，一节点上可能就有 30GB 内存是这个空间。前面提到的 young gc 中，如果某个对象连续多次幸存下来，就会被移进 Old 空间内。而等到 Old 空间满，就会发生一次 old gc，把失效对象移除。 
听起来很美好的样子，但是这些都是有代价的！在 GC 发生的时候，JVM 需要暂停程序运行，以便自己追踪对象图收集全部失效对象。在这期间，其他一切都不会继续运行。请求没有响应，ping 没有应答，分片不会分配…… 
当然，young gc 一般来说执行极快，没太大影响。但是 old 空间那么大，稍慢一点的 gc 就意味着程序几秒乃至十几秒的不可用，这太危险了。 
JVM 本身对 gc 算法一直在努力优化，Elasticsearch 也尽量复用内部对象，复用网络缓冲，然后还提供像 Doc Values 这样的特性。但不管怎么说，gc 性能总是我们需要密切关注的数据，因为它是集群稳定性最大的影响因子。 
如果你的 ES 集群监控里发现经常有很耗时的 GC，说明集群负载很重，内存不足。严重情况下，这些 GC 导致节点无法正确响应集群之间的 ping ，可能就直接从集群里退出了。然后


坑：ES 节点插件必须一致，否则重启后 es节点可能无法加入
